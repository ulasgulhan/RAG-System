from langchain_cohere import ChatCohere
from langchain_cohere.rag_retrievers import CohereRagRetriever
from langchain_cohere import CohereEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from .template import get_template
from dotenv import load_dotenv
import os
import pandas as pd
import numpy as np
from renumics import spotlight
import hashlib
import json
from langchain_core.documents import Document
from typing import List

load_dotenv()

COHERE_API_KEY = os.getenv("COHERE_API_KEY")


class LlmAgent:
    def __init__(self):
        self.index_name = os.getenv("INDEX_NAME")
        self.cohere_chat_model = ChatCohere(cohere_api_key=COHERE_API_KEY)
        self.cohere_embeddings = CohereEmbeddings(
            cohere_api_key=COHERE_API_KEY, model="embed-multilingual-v3.0"
        )
        self.rag = CohereRagRetriever(llm=self.cohere_chat_model)
        self.db = Chroma(
            collection_name="docs_store", embedding_function=self.cohere_embeddings
        )

    def format_docs(self, docs: List[Document]) -> str:
        """
        Formats a list of documents into a string for display.

        Parameters:
        docs (list): A list of Document objects.

        Returns:
        str: A formatted string of documents.
        """
        return "\n\n".join(
            f"Content: {doc.page_content}\nSource: {doc.metadata['source']}"
            for doc in docs
        )

    def stable_hash(self, doc: Document) -> str:
        """
        Generates a stable hash for a document based on its metadata.

        Parameters:
        doc (Document): The Document object to hash.

        Returns:
        str: A SHA-1 hash of the document's metadata.
        """
        return hashlib.sha1(
            json.dumps(doc.metadata, sort_keys=True).encode()
        ).hexdigest()

    def get_relevent_data(self, chunks: list, user_query: str) -> str:
        """
        Retrieves relevant data based on a user query using RAG (Retrieval-Augmented Generation).

        Parameters:
        chunks (list): A list of Document chunks.
        user_query (str): The user's query.

        Returns:
        str: The answer generated by the language model.
        """

        prompt = get_template()

        split_ids = list(map(self.stable_hash, chunks))

        # Add the document chunks to the Chroma database
        self.db.add_documents(chunks, ids=split_ids)

        input_docs = self.db.as_retriever(search_kwargs={"k": 20})

        # Create a RAG chain from documents
        rag_chain_from_docs = (
            RunnablePassthrough.assign(
                source_documents=(lambda x: self.format_docs(x["source_documents"]))
            )
            | prompt
            | self.cohere_chat_model
            | StrOutputParser()
        )
        rag_chain = RunnableParallel(
            {
                "source_documents": input_docs,
                "question": RunnablePassthrough(),
            }
        ).assign(answer=rag_chain_from_docs)

        response = rag_chain.invoke(user_query)

        answer = response["answer"]

        # Get the response data from the database
        response_data = self.db.get(include=["metadatas", "documents", "embeddings"])

        self.visualize_data(response_data, answer, user_query)

        return answer

    def visualize_data(self, response: dict, answer: str, question: str):
        """
        Visualizes the response data using a DataFrame and calculates distances for embeddings.

        Parameters:
        response (dict): The response data from the database.
        answer (str): The answer generated by the language model.
        question (str): The user's question.
        """
        df = pd.DataFrame(
            {
                "id": response["ids"],
                "source": [
                    metadata.get("source") for metadata in response["metadatas"]
                ],
                "page": [
                    metadata.get("page", -1) for metadata in response["metadatas"]
                ],
                "document": response["documents"],
                "embedding": response["embeddings"],
            }
        )

        question_row = pd.DataFrame(
            {
                "id": ["question"],
                "embedding": [self.cohere_embeddings.embed_query(question)],
            }
        )
        answer_row = pd.DataFrame(
            {
                "id": ["answer"],
                "embedding": [self.cohere_embeddings.embed_query(answer)],
            }
        )
        df = pd.concat([question_row, answer_row, df], ignore_index=True)

        question_embedding = self.cohere_embeddings.embed_query(question)

        # Calculate the distance of each embedding from the question embedding
        df["dist"] = df.apply(
            lambda row: np.linalg.norm(np.array(row["embedding"]) - question_embedding),
            axis=1,
        )

        spotlight.show(df)
